{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAKknWX8uRVK"
      },
      "source": [
        "Importing stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "S8HedO21uK1B"
      },
      "outputs": [],
      "source": [
        "import cupy as np\n",
        "import torchvision.datasets as ds\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "EPS = 1e-5\n",
        "np.random.seed(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpKqNpKquW9U"
      },
      "source": [
        "Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7FIxY8BOuaYR"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "    def feedforward(self, input):\n",
        "        raise NotImplementedError\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        raise NotImplementedError\n",
        "    def reset(self):\n",
        "        pass\n",
        "    def transform(self):\n",
        "        pass\n",
        "    def batchNormalization(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gfaQmHrucO7"
      },
      "source": [
        "Other Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hR59U0doun0D"
      },
      "outputs": [],
      "source": [
        "class BufferLayer(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        assert input_size == output_size\n",
        "        super().__init__(input_size, output_size)\n",
        "    def feedforward(self, input):\n",
        "        return input\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        return grad_output\n",
        "\n",
        "class DenseLayer(Layer):\n",
        "    def __init__(self, input_size, output_size, optimizer = 'Adam'):\n",
        "        super().__init__(input_size, output_size)\n",
        "        self.standard_deviation = 1\n",
        "        self.standard_deviation = np.sqrt(2 / (input_size + output_size))\n",
        "        self.weights = np.random.randn(input_size + 1, output_size) * self.standard_deviation\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "\n",
        "        self.m = np.zeros((input_size + 1, output_size))\n",
        "        self.v = np.zeros((input_size + 1, output_size))\n",
        "        self.t = 0\n",
        "        self.eps = 1e-8\n",
        "        self.opt = None\n",
        "        if optimizer == 'Adam':\n",
        "            self.opt = self.Adam\n",
        "        elif optimizer == 'Nadam':\n",
        "            self.opt = self.Nadam\n",
        "        else:\n",
        "            self.opt = self.NoOpt\n",
        "\n",
        "\n",
        "    def feedforward(self, input):\n",
        "        self.input = np.hstack((np.ones((input.shape[0], 1)), input))\n",
        "        self.output = self.input @ self.weights\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        grad_input = grad_output @ self.weights[1:,:].T\n",
        "        grad_weights = self.input.T @ grad_output\n",
        "        grad_weights = self.opt(grad_weights)\n",
        "        self.weights -= learning_rate * grad_weights\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.weights = np.random.randn(self.input_size + 1, self.output_size) * self.standard_deviation\n",
        "\n",
        "    def transform(self):\n",
        "        try:\n",
        "            self.weights.get()\n",
        "            self.weights = self.weights.get()\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def Adam(self, grad_weights):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad_weights\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad_weights * grad_weights\n",
        "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "\n",
        "        return m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "    def Nadam(self, grad_weights):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad_weights\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad_weights * grad_weights\n",
        "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "\n",
        "        return (self.beta1 * m_hat + (1 - self.beta1) * grad_weights) / (np.sqrt(v_hat) + self.eps)\n",
        "    def NoOpt(self, grad_weights):\n",
        "        return grad_weights\n",
        "\n",
        "class ReluLayer(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        assert input_size == output_size\n",
        "        super().__init__(input_size, output_size)\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def feedforward(self, input):\n",
        "        self.input = input\n",
        "        self.output = np.maximum(0, input)\n",
        "        return self.output\n",
        "\n",
        "    def relu_derivative(self, _x):\n",
        "        return 1. * (_x > 0)\n",
        "\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        return grad_output * self.relu_derivative(self.input)\n",
        "\n",
        "    def transform(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "class SigmoidLayer(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        assert input_size == output_size\n",
        "        super().__init__(input_size, output_size)\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        ret = np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
        "        return ret\n",
        "\n",
        "    def feedforward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.sigmoid(input)\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        return grad_output * (self.output) * (1 - self.output)\n",
        "\n",
        "    def transform(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "class SoftmaxLayer(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        assert input_size == output_size\n",
        "        super().__init__(input_size, output_size)\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def feedforward(self, input):\n",
        "        self.input = input\n",
        "        exp = np.exp(input - np.max(input, axis = 1).reshape(-1, 1))\n",
        "        self.output = exp / np.sum(exp, axis = 1).reshape(-1, 1)\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        return grad_output * (self.output - self.output * self.output)\n",
        "\n",
        "    def transform(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "\n",
        "class DropoutLayer(Layer):\n",
        "    def __init__(self, input_size, output_size, dropout_rate):\n",
        "        super().__init__(input_size, output_size)\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = None\n",
        "    def feedforward(self, input):\n",
        "        self.mask = np.random.binomial(1, 1 - self.dropout_rate, size = input.shape) / (1 - self.dropout_rate)\n",
        "        # print(self.mask)\n",
        "        return input * self.mask\n",
        "    def backpropagation(self, loss, learning_rate):\n",
        "        return loss * self.mask\n",
        "    def reset(self):\n",
        "        pass\n",
        "    def transform(self):\n",
        "        self.dropout_rate = 0\n",
        "        self.mask = None\n",
        "\n",
        "class BatchNormalizationLayer(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__(input_size, output_size)\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        self.gamma = 1\n",
        "        self.beta = 0\n",
        "        self.eps = EPS\n",
        "\n",
        "    def feedforward(self, input):\n",
        "        self.input = input\n",
        "        self.output = (input - np.mean(input, axis = 0)) / np.sqrt(np.var(input, axis = 0) + self.eps)\n",
        "        return self.gamma * self.output + self.beta\n",
        "\n",
        "    def backpropagation(self, grad_output, learning_rate):\n",
        "        grad_input = grad_output * self.gamma / np.sqrt(np.var(self.input, axis = 0) + self.eps)\n",
        "        grad_gamma = np.sum(grad_output * self.output, axis = 0)\n",
        "        grad_beta = np.sum(grad_output, axis = 0)\n",
        "        self.gamma -= learning_rate * grad_gamma\n",
        "        self.beta -= learning_rate * grad_beta\n",
        "        return grad_input\n",
        "\n",
        "    def reset(self):\n",
        "        self.gamma = 1\n",
        "        self.beta = 0\n",
        "\n",
        "    def transform(self):\n",
        "        self.input = None\n",
        "        self.output = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jSDeedGu88k"
      },
      "source": [
        "Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_vpk-7tSu-SU"
      },
      "outputs": [],
      "source": [
        "class SquaredLoss:\n",
        "    def feedforward(self, input, label):\n",
        "        return np.sum((input - label) ** 2)\n",
        "    def backpropagation(self, predicted, label):\n",
        "        return np.sum(2 * (predicted - label), axis = 0) / predicted.shape[0]\n",
        "class CrossEntropyLoss:\n",
        "    def feedforward(self, input, label):\n",
        "        return -np.sum(label * np.log2(input + EPS))\n",
        "\n",
        "    def backpropagation(self, predicted, label):\n",
        "        delta = -label / (predicted + EPS)\n",
        "        return delta\n",
        "\n",
        "class SoftmaxCrossEntropyLoss:\n",
        "    def feedforward(self, input, label):\n",
        "        return -np.sum(label * np.log2(input + EPS))\n",
        "\n",
        "    def backpropagation(self, predicted, label):\n",
        "        return predicted - label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHc4fw6JvMl_"
      },
      "source": [
        "Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "YM-n4rrVvQ5g"
      },
      "outputs": [],
      "source": [
        "class FNN:\n",
        "    def __init__(self, input_size, init_layer = BufferLayer, Loss = SoftmaxCrossEntropyLoss):\n",
        "        np.random.seed(100)\n",
        "        self.input_size = input_size\n",
        "        self.layers = [init_layer(input_size, input_size)]\n",
        "        self.LossModel = Loss()\n",
        "    def add_layer(self, Layer, nodes = None, **kwargs):\n",
        "        if nodes == None:\n",
        "            nodes = self.layers[-1].output_size\n",
        "        if Layer == DropoutLayer:\n",
        "            try:\n",
        "                dropout_rate = kwargs[\"dropout_rate\"]\n",
        "            except:\n",
        "                dropout_rate = 0.5\n",
        "            self.layers.append(Layer(self.layers[-1].output_size, nodes, dropout_rate))\n",
        "        elif Layer == DenseLayer:\n",
        "            try:\n",
        "                optimizer = kwargs[\"optimizer\"]\n",
        "            except:\n",
        "                optimizer = 'Adam'\n",
        "            self.layers.append(Layer(self.layers[-1].output_size, nodes, optimizer))\n",
        "        else:\n",
        "            self.layers.append(Layer(self.layers[-1].output_size, nodes))\n",
        "\n",
        "    def feedforward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.feedforward(input)\n",
        "        return input\n",
        "    def loss(self, output, label):\n",
        "        return self.LossModel.feedforward(output, label)\n",
        "\n",
        "    def backpropagation(self, loss, learning_rate):\n",
        "        n = len(self.layers)\n",
        "        if self.LossModel.__class__ == SoftmaxCrossEntropyLoss and self.layers[-1].__class__ == SoftmaxLayer:\n",
        "            n -= 1\n",
        "        for i in range(n - 1, 0, -1):\n",
        "            loss = self.layers[i].backpropagation(loss, learning_rate)\n",
        "\n",
        "    def trainUtil(self, train_data, train_label, epochs, learning_rate, minibatch, verbose = False):\n",
        "        self.reset()\n",
        "        decay = learning_rate / (epochs + 1)\n",
        "        losses = []\n",
        "        f1_macros = []\n",
        "        multiplier = 0.95\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, train_data.shape[0], minibatch):\n",
        "                output = self.feedforward(train_data[i:i + minibatch])\n",
        "                self.backpropagation(self.LossModel.backpropagation(output, train_label[i:i + minibatch]), learning_rate)\n",
        "\n",
        "            loss = self.loss(self.feedforward(train_data), train_label)\n",
        "            losses.append(loss)\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
        "\n",
        "            learning_rate -= decay\n",
        "            # learning_rate *= multiplier\n",
        "        return losses\n",
        "\n",
        "    def train(self, train_data, train_label, valid_data, valid_label, minibatch = 64, epochs = 1000, max_learning_rate = 5e-3, verbose = False):\n",
        "        if self.LossModel.__class__ == SoftmaxCrossEntropyLoss:\n",
        "            assert self.layers[-1].__class__ == SoftmaxLayer, \"The last layer must be SoftmaxLayer\"\n",
        "        lo = 0\n",
        "        hi = max_learning_rate\n",
        "        #ternary search\n",
        "        max_iter = 0\n",
        "        while hi - lo > 1e-6 and max_iter > 0:\n",
        "            max_iter -= 1\n",
        "            mid1 = lo + (hi - lo) / 3\n",
        "            mid2 = hi - (hi - lo) / 3\n",
        "            self.trainUtil(train_data, train_label, epochs, mid1, minibatch)\n",
        "            loss_1, accuracy_1, f1_macro_1 = self.getResult(valid_data, valid_label)\n",
        "            self.trainUtil(train_data, train_label, epochs, mid2, minibatch)\n",
        "            loss_2, accuracy_2, f1_macro_2 = self.getResult(valid_data, valid_label)\n",
        "            if f1_macro_1 > f1_macro_2:\n",
        "                hi = mid2\n",
        "            else:\n",
        "                lo = mid1\n",
        "\n",
        "        losses = self.trainUtil(train_data, train_label, epochs, hi, minibatch, verbose = verbose)\n",
        "        losses = np.array(losses)\n",
        "        try:\n",
        "          losses.get()\n",
        "          losses = losses.get()\n",
        "        except:\n",
        "          pass\n",
        "        return losses\n",
        "\n",
        "    def reset(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset()\n",
        "\n",
        "    def predict(self, input):\n",
        "        input = np.array(input)\n",
        "        input = input.reshape(-1, self.input_size)\n",
        "        output = self.feedforward(input)\n",
        "        return output\n",
        "\n",
        "    def predictClass(self, input):\n",
        "        output = self.predict(input)\n",
        "        output = np.argmax(output, axis = 1)\n",
        "        output = output.reshape(-1, 1)\n",
        "        output = output + 1\n",
        "        return output\n",
        "\n",
        "\n",
        "    def transform(self):\n",
        "        for layer in self.layers:\n",
        "            layer.transform()\n",
        "\n",
        "    def save(self, filename):\n",
        "        self.transform()\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "\n",
        "    def getConfusionMatrix(self, output, label):\n",
        "        confusion_matrix = np.zeros((26, 26))\n",
        "        for i in range(label.shape[0]):\n",
        "            confusion_matrix[int(label[i]) - 1][int(output[i]) - 1] += 1\n",
        "        return confusion_matrix\n",
        "\n",
        "    def printConfusionMatrix(self, matrix):\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(\"Letters ↓\", end = \" \")\n",
        "        #print TP TN FP FN for each letter in the confusion matrix in a nice format with fixed width\n",
        "        print(\"TP\".rjust(4), \"TN\".rjust(4), \"FP\".rjust(4), \"FN\".rjust(4))\n",
        "        for i in range(26):\n",
        "            print('   ',chr(ord('A') + i), end = \"    \")\n",
        "            TP = matrix[i][i]\n",
        "            TN = np.sum(matrix) - np.sum(matrix[i]) - np.sum(matrix[:, i]) + matrix[i][i]\n",
        "            FP = np.sum(matrix[:, i]) - matrix[i][i]\n",
        "            FN = np.sum(matrix[i]) - matrix[i][i]\n",
        "            print(str(int(TP)).rjust(4), str(int(TN)).rjust(4), str(int(FP)).rjust(4), str(int(FN)).rjust(4))\n",
        "\n",
        "\n",
        "    def getResult(self, input, label, printConfusionMatrix = False):\n",
        "        output = self.feedforward(input)\n",
        "        prediction = np.argmax(output, axis = 1) + 1\n",
        "        loss = self.loss(output, label)\n",
        "        label = np.argmax(label, axis = 1) + 1\n",
        "\n",
        "        confusion_matrix = self.getConfusionMatrix(prediction, label)\n",
        "        if printConfusionMatrix:\n",
        "            self.printConfusionMatrix(confusion_matrix)\n",
        "        accuracy = np.sum(prediction == label) / label.shape[0] * 100\n",
        "        f1_score = np.zeros((26))\n",
        "        for i in range(26):\n",
        "            f1_score[i] = 2 * confusion_matrix[i][i] / (np.sum(confusion_matrix[i]) + np.sum(confusion_matrix[:, i]))\n",
        "\n",
        "        f1_macro = np.mean(f1_score)\n",
        "\n",
        "        return loss, accuracy, f1_macro\n",
        "\n",
        "    def adjustDropOut(self):\n",
        "        for layer in self.layers:\n",
        "            if layer.__class__ == DropoutLayer:\n",
        "                layer.transform()\n",
        "\n",
        "    def printResult(self, input, label):\n",
        "        loss, accuracy, f1_macro = self.getResult(input, label)\n",
        "        print(\"=========================================\")\n",
        "        print(f\"Loss = {loss}\")\n",
        "        print(f\"Accuracy = {accuracy}%\")\n",
        "        print(f\"F1 Macro = {f1_macro}\")\n",
        "        print(\"=========================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models"
      ],
      "metadata": {
        "id": "Uslv0lbh4_Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model 3 0.005 good. After that i would choose model 2\n",
        "# def model1(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512)\n",
        "#     model.add_layer(SigmoidLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, 256)\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.2)\n",
        "#     model.add_layer(DenseLayer, 128)\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DenseLayer, output_size)\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model\n",
        "\n",
        "# def model2(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512, optimizer='Nadam')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, 256, optimizer='Nadam')\n",
        "#     model.add_layer(SigmoidLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.2)\n",
        "#     model.add_layer(BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 128, optimizer='Nadam')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DenseLayer, output_size, optimizer='Nadam')\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model\n",
        "\n",
        "# def model3(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BufferLayer)\n",
        "#     model.add_layer(DenseLayer, 512, optimizer='None')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, 128, optimizer='None')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.2)\n",
        "#     model.add_layer(DenseLayer, 128, optimizer='None')\n",
        "#     model.add_layer(SigmoidLayer)\n",
        "#     model.add_layer(DenseLayer, output_size, optimizer='None')\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "ES1efu7045wD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #model 2 0.001 good. Model 3 less overfit\n",
        "# def model1(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512)\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, 256)\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(BatchNormalizationLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.3)\n",
        "#     model.add_layer(DenseLayer, 256)\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DenseLayer, output_size)\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model\n",
        "\n",
        "# def model2(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512, optimizer='Nadam')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, 256, optimizer='Nadam')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.3)\n",
        "#     model.add_layer(BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 256, optimizer='Nadam')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DenseLayer, output_size, optimizer='Nadam')\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model\n",
        "\n",
        "# def model3(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512, optimizer='None')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, 256, optimizer='None')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(BatchNormalizationLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.3)\n",
        "#     model.add_layer(DenseLayer, 256, optimizer='None')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DenseLayer, output_size, optimizer='None')\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "ERQxOmRzMFwM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best model: Model 3 with learning rate = 0.005\n",
        "# def model1(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512)\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, output_size)\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model\n",
        "\n",
        "# def model2(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512, optimizer='Nadam')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, output_size, optimizer='Nadam')\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model\n",
        "\n",
        "# def model3(input_size, output_size):\n",
        "#     model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "#     model.add_layer(DenseLayer, 512, optimizer='None')\n",
        "#     model.add_layer(ReluLayer)\n",
        "#     model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "#     model.add_layer(DenseLayer, output_size, optimizer='None')\n",
        "#     model.add_layer(SoftmaxLayer)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "htywfDQZXYHq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Pick\n",
        "def model1(input_size, output_size):\n",
        "    model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "    model.add_layer(DenseLayer, 512, optimizer='Nadam')\n",
        "    model.add_layer(ReluLayer)\n",
        "    model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "    model.add_layer(DenseLayer, output_size, optimizer='Nadam')\n",
        "    model.add_layer(SoftmaxLayer)\n",
        "    return model\n",
        "\n",
        "def model2(input_size, output_size):\n",
        "    model = FNN(input_size, init_layer=BatchNormalizationLayer)\n",
        "    model.add_layer(DenseLayer, 512)\n",
        "    model.add_layer(ReluLayer)\n",
        "    model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "    model.add_layer(DenseLayer, 256)\n",
        "    model.add_layer(ReluLayer)\n",
        "    model.add_layer(BatchNormalizationLayer)\n",
        "    model.add_layer(DropoutLayer, dropout_rate = 0.3)\n",
        "    model.add_layer(DenseLayer, 128)\n",
        "    model.add_layer(ReluLayer)\n",
        "    model.add_layer(DenseLayer, output_size)\n",
        "    model.add_layer(SoftmaxLayer)\n",
        "    return model\n",
        "\n",
        "def model3(input_size, output_size):\n",
        "    model = FNN(input_size, init_layer=BufferLayer)\n",
        "    model.add_layer(DenseLayer, 512, optimizer='None')\n",
        "    model.add_layer(ReluLayer)\n",
        "    model.add_layer(DropoutLayer, dropout_rate = 0.5)\n",
        "    model.add_layer(DenseLayer, 256, optimizer='None')\n",
        "    model.add_layer(ReluLayer)\n",
        "    model.add_layer(DropoutLayer, dropout_rate = 0.2)\n",
        "    model.add_layer(DenseLayer, 128, optimizer='None')\n",
        "    model.add_layer(SigmoidLayer)\n",
        "    model.add_layer(DenseLayer, output_size, optimizer='None')\n",
        "    model.add_layer(SoftmaxLayer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "yso1-dqXaqr1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhtfE4VQvTN2"
      },
      "source": [
        "Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "y3qg3vn_vYCp",
        "outputId": "4638708b-c463-488b-cd92-acd0bbd87aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Model 2 with learning rate = 0.005\n",
            "Training result:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-963392abf6a9>\u001b[0m in \u001b[0;36m<cell line: 200>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test result:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;31m# test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-963392abf6a9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1805106.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training result:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation result:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintConfusionMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-8ced2d61ec66>\u001b[0m in \u001b[0;36mprintResult\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprintResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_macro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=========================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss = {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-8ced2d61ec66>\u001b[0m in \u001b[0;36mgetResult\u001b[0;34m(self, input, label, printConfusionMatrix)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintConfusionMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-8ced2d61ec66>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-0c383d86a146>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.__matmul__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.__array_ufunc__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cupy/_core/_gufuncs.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 ' but %s given' % (len(input_coredimss), len(args)))\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         args, dimsizess, loop_output_dims, outs, m_dims = self._get_args_transposed(  # NOQA\n\u001b[0m\u001b[1;32m    659\u001b[0m             args, input_axes, outs, output_axes)\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cupy/_core/_gufuncs.py\u001b[0m in \u001b[0;36m_get_args_transposed\u001b[0;34m(self, args, input_axes, outs, output_axes)\u001b[0m\n\u001b[1;32m    470\u001b[0m         shape = internal._broadcast_shapes(\n\u001b[1;32m    471\u001b[0m             [a.shape[:-len(self._input_coredimss)] for a in args])\n\u001b[0;32m--> 472\u001b[0;31m         args = [_manipulation.broadcast_to(\n\u001b[0m\u001b[1;32m    473\u001b[0m             a, shape + a.shape[-len(self._input_coredimss):]) for a in args]\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cupy/_core/_gufuncs.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    470\u001b[0m         shape = internal._broadcast_shapes(\n\u001b[1;32m    471\u001b[0m             [a.shape[:-len(self._input_coredimss)] for a in args])\n\u001b[0;32m--> 472\u001b[0;31m         args = [_manipulation.broadcast_to(\n\u001b[0m\u001b[1;32m    473\u001b[0m             a, shape + a.shape[-len(self._input_coredimss):]) for a in args]\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument 'array' has incorrect type (expected cupy._core.core._ndarray_base, got numpy.ndarray)"
          ]
        }
      ],
      "source": [
        "def read_data():\n",
        "    train_validation_dataset = ds.EMNIST(root='./data', split='letters',\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True)\n",
        "    independent_test_dataset = ds.EMNIST(root='./data',\n",
        "    split='letters',\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor())\n",
        "    return train_validation_dataset, independent_test_dataset\n",
        "\n",
        "def show(index, dataset):\n",
        "    image, label = dataset[index]\n",
        "    print(label)\n",
        "    print(image)\n",
        "    plt.imshow(image.reshape(28, 28))\n",
        "    plt.show()\n",
        "\n",
        "def preprocess(train_validation, test):\n",
        "    train, validation = model_selection.train_test_split(train_validation, test_size=0.15, random_state=42)\n",
        "    train_data = np.array([np.array(x[0]).flatten() for x in train])\n",
        "    train_labels = np.array([x[1] for x in train])\n",
        "    validation_data = np.array([np.array(x[0]).flatten() for x in validation])\n",
        "    validation_labels = np.array([x[1] for x in validation])\n",
        "    test_data = np.array([np.array(x[0]).flatten() for x in test])\n",
        "    test_labels = np.array([x[1] for x in test])\n",
        "\n",
        "    # train_data = train_data / 255\n",
        "    # validation_data = validation_data / 255\n",
        "    # test_data = test_data / 255\n",
        "\n",
        "    # train_data = preprocessing.scale(train_data)\n",
        "    # validation_data = preprocessing.scale(validation_data)\n",
        "    # test_data = preprocessing.scale(test_data)\n",
        "\n",
        "    try:\n",
        "        train_labels.reshape(-1, 1).get()\n",
        "        oneHotEncoder = preprocessing.OneHotEncoder().fit(train_labels.reshape(-1, 1).get())\n",
        "        train_labels = oneHotEncoder.transform(train_labels.reshape(-1, 1).get()).toarray()\n",
        "        validation_labels = oneHotEncoder.transform(validation_labels.reshape(-1, 1).get()).toarray()\n",
        "        test_labels = oneHotEncoder.transform(test_labels.reshape(-1, 1).get()).toarray()\n",
        "\n",
        "        train_labels = np.array(train_labels)\n",
        "        validation_labels = np.array(validation_labels)\n",
        "        test_labels = np.array(test_labels)\n",
        "    except:\n",
        "        oneHotEncoder = preprocessing.OneHotEncoder().fit(train_labels.reshape(-1, 1))\n",
        "        train_labels = oneHotEncoder.transform(train_labels.reshape(-1, 1)).toarray()\n",
        "        validation_labels = oneHotEncoder.transform(validation_labels.reshape(-1, 1)).toarray()\n",
        "        test_labels = oneHotEncoder.transform(test_labels.reshape(-1, 1)).toarray()\n",
        "\n",
        "    return train_data, train_labels, validation_data, validation_labels, test_data, test_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_validation, test = read_data()\n",
        "    train_data, train_labels, validation_data, validation_labels, test_data, test_labels = preprocess(train_validation, test)\n",
        "\n",
        "\n",
        "    m1 = model1(train_data.shape[1], train_labels.shape[1])\n",
        "    m2 = model2(train_data.shape[1], train_labels.shape[1])\n",
        "    m3 = model3(train_data.shape[1], train_labels.shape[1])\n",
        "    best_model = None\n",
        "    losses = []\n",
        "    models = [m1, m2, m3]\n",
        "    learning_rates = [5e-3, 1e-3, 5e-4, 1e-4]\n",
        "    model_losses = []\n",
        "    model_accuracy = []\n",
        "    model_f1_macro = []\n",
        "    best_learning_rate = None\n",
        "    best_model_index = None\n",
        "    best_f1 = 0\n",
        "    for model in models:\n",
        "        model_losses.append([])\n",
        "        model_accuracy.append([])\n",
        "        model_f1_macro.append([])\n",
        "        for learning_rate in learning_rates:\n",
        "            print(f\"Learning rate = {learning_rate}\")\n",
        "            loss = model.train(train_data, train_labels, validation_data, validation_labels, epochs = 10, max_learning_rate = learning_rate, verbose = False)\n",
        "            model.adjustDropOut()\n",
        "            losses.append(loss)\n",
        "            print(\"Training result:\")\n",
        "            model.printResult(train_data, train_labels)\n",
        "            print(\"Validation result:\")\n",
        "            model.printResult(validation_data, validation_labels)\n",
        "            res = model.getResult(validation_data, validation_labels)\n",
        "            model_losses[-1].append(res[0])\n",
        "            model_accuracy[-1].append(res[1])\n",
        "            model_f1_macro[-1].append(res[2])\n",
        "            if res[2] > best_f1:\n",
        "                best_learning_rate = learning_rate\n",
        "                best_model_index = len(model_losses) - 1\n",
        "                best_f1 = model_f1_macro[-1][-1]\n",
        "\n",
        "    best_model = None\n",
        "    if best_model_index == 0:\n",
        "      best_model = model1(train_data.shape[1], train_labels.shape[1])\n",
        "    if best_model_index == 1:\n",
        "      best_model = model2(train_data.shape[1], train_labels.shape[1])\n",
        "    if best_model_index == 2:\n",
        "      best_model = model3(train_data.shape[1], train_labels.shape[1])\n",
        "\n",
        "\n",
        "    #plot 4 different learning rates in one graph\n",
        "    for i in range (len(models)):\n",
        "        plt.figure()\n",
        "        plt.title(f\"Loss vs Epochs (Model {i + 1})\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        for j in range(len(learning_rates)):\n",
        "            plt.plot(losses[i * len(learning_rates) + j], label = f\"Learning rate = {learning_rates[j]}\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"model{i + 1}.png\")\n",
        "        plt.show()\n",
        "\n",
        "    #plot 3 different models in one graph (learning rate vs loss, accuracy, f1_macro)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Learning rate vs Loss\")\n",
        "    plt.xlabel(\"Learning rate\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    learning_rates = np.array(learning_rates)\n",
        "    try:\n",
        "        learning_rates.get()\n",
        "        learning_rates = learning_rates.get()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        model_losses[i] = np.array(model_losses[i])\n",
        "        try:\n",
        "            model_losses[i].get()\n",
        "            model_losses[i] = model_losses[i].get()\n",
        "        except:\n",
        "            pass\n",
        "        plt.plot(learning_rates, model_losses[i], label = f\"Model {i + 1}\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig(\"loss.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Learning rate vs Accuracy\")\n",
        "    plt.xlabel(\"Learning rate\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    for i in range(len(models)):\n",
        "        model_accuracy[i] = np.array(model_accuracy[i])\n",
        "        try:\n",
        "            model_accuracy[i].get()\n",
        "            model_accuracy[i] = model_accuracy[i].get()\n",
        "        except:\n",
        "            pass\n",
        "        plt.plot(learning_rates, model_accuracy[i], label = f\"Model {i + 1}\")\n",
        "\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig(\"accuracy.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Learning rate vs F1 Macro\")\n",
        "    plt.xlabel(\"Learning rate\")\n",
        "    plt.ylabel(\"F1 Macro\")\n",
        "    for i in range(len(models)):\n",
        "        model_f1_macro[i] = np.array(model_f1_macro[i])\n",
        "        try:\n",
        "            model_f1_macro[i].get()\n",
        "            model_f1_macro[i] = model_f1_macro[i].get()\n",
        "        except:\n",
        "            pass\n",
        "        plt.plot(learning_rates, model_f1_macro[i], label = f\"Model {i + 1}\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"f1_macro.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(f\"Best model: Model {best_model_index} with learning rate = {best_learning_rate}\")\n",
        "    best_model.train(train_data, train_labels, validation_data, validation_labels, epochs = 200, max_learning_rate = best_learning_rate, verbose = False)\n",
        "    best_model.adjustDropOut()\n",
        "    best_model.save(\"1805106.pkl\")\n",
        "    print(\"Training result:\")\n",
        "    best_model.printResult(train_data, train_labels)\n",
        "    print(\"Validation result:\")\n",
        "    best_model.printResult(validation_data, validation_labels, printConfusionMatrix = True)\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    train_validation, test = read_data()\n",
        "    train_data, train_labels, validation_data, validation_labels, test_data, test_labels = preprocess(train_validation, test)\n",
        "    model = FNN.load(\"1805106.pkl\")\n",
        "    print(\"Finish loading\")\n",
        "    print(\"Training result:\")\n",
        "    model.printResult(train_data, train_labels)\n",
        "    print(\"Validation result:\")\n",
        "    model.printResult(validation_data, validation_labels)\n",
        "    print(\"Test result:\")\n",
        "    model.printResult(test_data, test_labels)\n",
        "main()\n",
        "# test()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}